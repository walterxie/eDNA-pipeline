{"name":"eDNA-pipeline","tagline":"eDNA data pipeline 2015","body":"# eDNA-pipeline\r\n\r\nAlexei J Drummond, Richard D Newcomb, Thomas R Buckley, Dong Xie, Andrew Dopheide, Benjamin CM Potter, \r\nJoseph Heled, Howard A Ross, Leah Tooman, Stefanie Grosser, Duckchul Park, Nicholas J Demetras, \r\nMark I Stevens, James C Russell, Sandra H Anderson, Anna Carter and Nicola Nelson, (2015): \r\n\"Evaluating a multigene environmental DNA approach for biodiversity assessment\", GigaScience.\r\n\r\nThe online version of this article (doi:[10.1186/s13742-015-0086-1](http://dx.doi.org/10.1186/s13742-015-0086-1)) or http://link.springer.com/article/10.1186/s13742-015-0086-1\r\n\r\nSupporting data in GigaScience Database: http://dx.doi.org/10.5524/100144\r\n\r\n## Data\r\n\r\n1. BioProject in NCBI:\r\n  http://www.ncbi.nlm.nih.gov/bioproject/?term=PRJNA267737\r\n\r\n2. 20 BioSamples (including lat-long, elevation, temperature):\r\n  http://www.ncbi.nlm.nih.gov/biosample?Db=biosample&DbFrom=bioproject&Cmd=Link&LinkName=bioproject_biosample&LinkReadableName=BioSample&ordinalpos=1&IdsFromResult=267737\r\n\r\n3. Soil environmental sequences (454) in SRA:\r\n  http://www.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?study=SRP050103\r\n\r\n4. Elevations:\r\n  plot_elevations.txt\r\n\r\n5. Invertebrate (leaf litter + pitfall traps):\r\n  CO1_Invertebrate_Pitfall_1526_OTUs.csv, CO1_Leaf_Litter_1526_OTUs.csv\r\n\r\n6. Vegetation survey data:\r\n  Hauturu (Little Barrier island) in Jan 2011 (http://nvs.landcareresearch.co.nz).\r\n  diameters_pilot_species_1_2_0.csv, saplings_pilot_species_1_2_0.csv, seedlings_pilot_species_1_2_0.csv\r\n\r\n7. Bird counts:\r\n  birds_pilot_species_1_2_0.csv\r\n\r\n8. SraRunTable.txt : \r\n  a SRA mapping file to map SRA code to subplot name.\r\n  \r\n9. Soil chemistry:\r\n  LJ12027.txt\r\n\r\n## Folder structure in working path \r\n\r\n1. A working folder, such as ./pipeline\r\n\r\n2. Folders for each data set (genes), such as ./pipeline/16S\r\n\r\n3. Folders for deconvolution, such as ./pipeline/16S/deconvoluted\r\n\r\n4. Folders for quality control, such as ./pipeline/16S/qc\r\n\r\n5. Folders for each OTU threshold, such as ./pipeline/16S/otus97\r\n\r\nFor example,\r\n```\r\npipeline\r\n |___ 16S\r\n |     |___ deconvoluted\r\n |     |___ qc\r\n |     |___ otu?? (e.g. 97)\r\n |___ 18S\r\n ...  \r\n```\r\n\r\n\r\n## Download 454 sequences \r\n\r\n1. Download soil environmental sequences (454) from SRA:\r\n  http://www.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?study=SRP050103\r\n\r\n2. Convert sra format into fastq using SRA Toolkit, such as:\r\n  ```\r\n  /sratoolkit.2.4.3-mac64/bin/fastq-dump SRR1720812.sra\r\n  ```\r\n  SRR1720812.fastq will be ready in the same folder.\r\n\r\n3. Combine all fastq files from the same marker into one file:\r\n  ```\r\n  cat *.fastq > 16S.fastq\r\n  ```\r\n  Alternatively, use scripts *downloadData.sh* and *prepareData.sh* as discribed below.\r\n\r\n\r\n## UPARSE \r\n\r\n1. Download USEARCH (http://www.drive5.com/usearch/download.html)\r\n\r\n2. Setup USEARCH by copying it to /Applications, and create a link:\r\n  ```\r\n  ln -s usearch8.0.??? usearch8\r\n  ```\r\n\r\n3. Create a working folder, such as ./pipeline\r\n\r\n4. Copy all scripts in to working folder, such as pipeline/scripts\r\n\r\n5. Download data\r\n  ```\r\n  scripts/downloadData.sh \r\n  scripts/prepareData.sh\r\n  ```\r\n\r\n6. We strongly recommend to use error corrector for 454 data. In this pipeline, \r\nwe choose Acacia (http://www.nature.com/nmeth/journal/v9/n5/abs/nmeth.1990.html). \r\nPlease install Acacia and change its path in the script *pipelineDerep.sh* before go to step 7.\r\nNote that you need to give enough memory according to the size of the largest cluster from your data. \r\nWe recommend to assign at least 50 GB memory to Acacia (-Xmx50g) for our 16S and 10 GB for the rest of datasets. \r\n\r\n7. Go into each gene folder to run script for either denovo chimera filtering or reference filtering (16S only), e.g.\r\n  ```\r\n  cd 16S\r\n  ../scripts/runOTUsRef.sh ./deconvoluted/16S.fastq \r\n  ```\r\n  or \r\n  ```\r\n  cd COI\r\n  ../scripts/runOTUsDenovo.sh ./deconvoluted/COI.fastq \r\n  ```\r\n  Note: the reference dataset gold.fa (http://www.drive5.com/usearch/manual/cmd_uchime_ref.html) is required by *runOTUsRef.sh*.\r\n\r\n\r\n## Generate Community Matrix \r\n\r\nNote: USEARCH 8 mixes all sample-based data into one pool,\r\ntherefore the sample information of each duplicate read would be lost during \r\nde-replication process. In *createAllCommunityMatrix.r*, we retrive the sample of each duplicate \r\nread from the mapping file derep.uc created by a modified command:\r\n```\r\n$USEARCH -derep_fulllength ./qc/denoised.fasta -fastaout ./qc/derep.fasta -sizeout -uc ./qc/derep.uc\r\n```\r\nto create the community matrix retaining the correct reads' distribution of samples.   \r\n\r\n\r\n##  BLAST \r\n\r\nUse *cleanSizeAnnotation.sh* to clean up the size annotation in the sequence label created by USEARCH, \r\nwhich will cause MEGAN input error. \r\n\r\n\r\n## Community Matrix Analysis \r\n\r\nThe detail is in [R/README.md](R/README.md).\r\n\r\n1. Run *createAllCommunityMatrix.r* first to create community matrices.\r\n\r\n2. Run *createAllDiversitiesOTUsTable.r* second to get the rarefaction table and OTU threshold table. This is time-consuming.\r\nIf you do not need clustering through different thresholds, you could use faster script *createAllRarefactionTable.r* to generate \r\nthe rarefaction table at 97% threshold only. \r\n\r\n3. Change source path for pipeline and working path for data into your local path. Run *createAllFiguresTables.r* to get all figures and tables.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}